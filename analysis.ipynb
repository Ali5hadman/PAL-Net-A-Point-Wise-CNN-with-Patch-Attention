{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "col = []\n",
    "df = pd.read_csv('5-fold-cv/results_5.csv', header=None)\n",
    "for i in range(int(len(df.columns)/2)):\n",
    "    col.append(\"fold_\" + str(i+1)+\"_closes\")\n",
    "    col.append(\"fold_\" + str(i+1)+\"_average\")\n",
    "\n",
    "df.columns = col\n",
    "\n",
    "df_colest = pd.DataFrame()\n",
    "df_average = pd.DataFrame()\n",
    "\n",
    "for i in range(len(df.columns)):\n",
    "    if i % 2 == 0:\n",
    "        df_colest[df.columns[i]] = df[df.columns[i]]\n",
    "    else:\n",
    "        df_average[df.columns[i]] = df[df.columns[i]]\n",
    "\n",
    "print(df_colest.mean())\n",
    "print()\n",
    "print(df_average.mean())\n",
    "print()\n",
    "\n",
    "print(\"closest mean:\", df_colest.mean().mean())\n",
    "print()\n",
    "print(\"average mean:\",df_average.mean().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd6a9d",
   "metadata": {},
   "source": [
    "## Point-wise analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578fcedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### names and data\n",
    "\n",
    "landmark_names = [\n",
    " 'Trichion (Tr)',\n",
    " 'Glabella (G)',\n",
    " 'Nasion (N)',\n",
    " 'Pronasale (Prn)',\n",
    " 'Columella (C)',\n",
    " 'Subnasale (Sn)',\n",
    " 'Labiale Superius (Ls)',\n",
    " 'Stomion (Sto)',\n",
    " 'Labiale Inferius (Li)',\n",
    " 'Sublabiale (Sl)',\n",
    " 'Pogonion (Pg)',\n",
    " 'Gnathion (Gn)',\n",
    "\n",
    " 'Tragion  Right (T)',\n",
    " 'Preaurale  Right (Pra)',\n",
    " 'Superaurale  Right (Sa)',\n",
    " 'Postaurale  Right (Pa)',\n",
    " 'Subaurale  Right (Sba)',\n",
    " 'Frontotemporale  Right (Ft)',\n",
    " 'Zygion  Right (Zy)',\n",
    " 'Gonion  Right (Go)',\n",
    " 'Incisura Sovraorbitaria  Right (Is)',\n",
    " 'Exocanthion  Right (Ex)',\n",
    " 'Orbitale  Right (Or)',\n",
    " 'Endocanthion  Right (En)',\n",
    " 'Malare Cheek  Right (Chk)',\n",
    " 'Cresta Alare  Right (Ac)',\n",
    " 'Alare  Right (Al)',\n",
    " 'Terminale Inferiore della Narice  Right (Itn)',\n",
    " 'Terminale Superiore della Narice  Right (Stn)',\n",
    " 'Crista Philtri  Right (Cph)',\n",
    " 'Cheilion  Right (Ch)',\n",
    "\n",
    " 'Tragion  Left (T)',\n",
    " 'Preaurale  Left (Pra)',\n",
    " 'Superaurale  Left (Sa)',\n",
    " 'Postaurale  Left (Pa)',\n",
    " 'Subaurale  Left (Sba)',\n",
    " 'Frontotemporale  Left (Ft)',\n",
    " 'Zygion  Left (Zy)',\n",
    " 'Gonion  Left (Go)',\n",
    " 'Incisura Sovraorbitaria  Left (Is)',\n",
    " 'Exocanthion  Left (Ex)',\n",
    " 'Orbitale  Left (Or)',\n",
    " 'Endocanthion  Left (En)',\n",
    " 'Malare Cheek  Left (Chk)',\n",
    " 'Cresta Alare  Left (Ac)',\n",
    " 'Alare  Left (Al)',\n",
    " 'Terminale Inferiore della Narice  Left (Itn)',\n",
    " 'Terminale Superiore della Narice  Left (Stn)',\n",
    " 'Crista Philtri  Left (Cph)',\n",
    " 'Cheilion  Left (Ch)']\n",
    "\n",
    "\n",
    "intra_observer= [1.927591405684143,\n",
    " 2.1574139811555186,\n",
    " 2.2195466249502784,\n",
    " 1.7978395106150071,\n",
    " 1.8977360287459006,\n",
    " 1.8583915065557626,\n",
    " 1.926574364112194,\n",
    " 1.7491419943493998,\n",
    " 2.0244456358698697,\n",
    " 2.388143892439652,\n",
    " 2.2083615105160725,\n",
    " 2.2621451418506013,\n",
    " 2.1539528455396946,\n",
    " 3.0503314379854305,\n",
    " 2.8948614620329645,\n",
    " 2.9862849585425604,\n",
    " 2.6486181179605266,\n",
    " 2.0027540020455774,\n",
    " 1.8334583210140951,\n",
    " 1.8391280707666777,\n",
    " 1.9185734330143707,\n",
    " 2.2287472953087155,\n",
    " 1.8800039502550994,\n",
    " 1.9160683795570361,\n",
    " 3.0887628262576685,\n",
    " 1.9053817809029936,\n",
    " 2.0446244443180928,\n",
    " 2.229553342888491,\n",
    " 1.913046351522015,\n",
    " 2.0192582366725587,\n",
    " 1.8982496437515557,\n",
    " 2.289694465878702,\n",
    " 3.580206931352167,\n",
    " 3.2123263137159257,\n",
    " 3.083268165921801,\n",
    " 3.276776071912053,\n",
    " 1.9481910519896897,\n",
    " 1.887633378484561,\n",
    " 2.005910295138464,\n",
    " 1.7899956329170998,\n",
    " 2.3017080306344804,\n",
    " 1.928489475565335,\n",
    " 1.9563784072490022,\n",
    " 3.5377327744631595,\n",
    " 2.3311023630965413,\n",
    " 2.525872342423564,\n",
    " 2.1623337012671238,\n",
    " 1.959393971820371,\n",
    " 1.966092809259711,\n",
    " 1.9135527586632481]\n",
    "\n",
    "\n",
    "df_final = pd.DataFrame()\n",
    "df_final['landmark'] = landmark_names\n",
    "df_final['intra_observer'] = intra_observer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08577feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easy computation\n",
    "df_calculate = df_average.copy()\n",
    "\n",
    "\n",
    "# Compute mean and std deviation\n",
    "df_calculate['mean'] = df_calculate.mean(axis=1)\n",
    "df_calculate['std'] = df_calculate.std(axis=1)\n",
    "\n",
    "# Format output with mean ± std\n",
    "df_calculate['mean±std'] = df_calculate.apply(lambda row: f\"{row['mean']:.4f} ± {row['std']:.4f}\", axis=1)\n",
    "\n",
    "df_final['mean±std'] = df_calculate['mean±std']\n",
    "df_final[\"intra_observer\"] = df_final[\"intra_observer\"].round(4)\n",
    "\n",
    "# df_final.index = df_final['landmark']\n",
    "# df_final\n",
    "# print(df_final.to_latex(index=False, float_format=\"%.4f\"))\n",
    "# df_final['intra_observer'].astype(float).mean()\n",
    "\n",
    "print(f\"{df_calculate['std'].mean():.4f}\")\n",
    "# df_final\n",
    "df_calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0664f5d5",
   "metadata": {},
   "source": [
    "## Distance-wise analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d42952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "distance_wise_average = np.load('5-fold-cv/distance_wise_average.npy')\n",
    "distance_wise_closest = np.load('5-fold-cv/distance_wise_closes.npy')\n",
    "\n",
    "point_wise_average = np.load('5-fold-cv/point_wise_average.npy')\n",
    "point_wise_closest = np.load('5-fold-cv/point_wise_closes.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aeedd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(distance_wise_closest, axis = 0).shape\n",
    "np.std(distance_wise_closest, axis = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee066639",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_wise_average.shape\n",
    "for i in range(len(distance_wise_average)):\n",
    "    print(f\"distance_wise_average {i+1}:\", np.mean(distance_wise_average[i]))\n",
    "    # print(f\"distance_wise_closest {i+1}:\", np.mean(distance_wise_closest[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e058cd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_neighborhood_variability_matrix(variability_matrix):\n",
    "    \"\"\"\n",
    "    Plots the neighborhood variability matrix as a heatmap. \n",
    "    variability_matrix: numpy array of shape (50, 50)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(variability_matrix, cmap='coolwarm', annot=False, cbar=True, \n",
    "                xticklabels=landmark_names, yticklabels=landmark_names)\n",
    "    plt.title('Landmark Pairwise Distance Accuracy')\n",
    "    plt.xlabel('Landmark Name')\n",
    "    plt.ylabel('Landmark Name')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "model_type = '5-fold-cv/'\n",
    "distance_wise_average = np.load(model_type +'distance_wise_average.npy')\n",
    "distance_wise_closest = np.load(model_type +'distance_wise_closes.npy')\n",
    "\n",
    "point_wise_average = np.load(model_type +'point_wise_average.npy')\n",
    "point_wise_closest = np.load(model_type +'point_wise_closes.npy')\n",
    "\n",
    "distance_error_matrix_mean_closest = np.mean(distance_wise_closest, axis = 0)\n",
    "distance_error_matrix_std_closest = np.std(distance_wise_closest, axis = 0)\n",
    "\n",
    "distance_error_matrix_mean_average = np.mean(distance_wise_average, axis = 0)\n",
    "distance_error_matrix_std_average = np.std(distance_wise_average, axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Distance-wise average: (closest)\")\n",
    "print('Mean: ' ,distance_error_matrix_mean_closest.mean())\n",
    "print('STD: ' ,distance_error_matrix_std_closest.mean())\n",
    "\n",
    "print()\n",
    "print(\"Distance-wise average: (average)\")\n",
    "print('Mean: ' ,distance_error_matrix_mean_average.mean())\n",
    "print('STD: ' ,distance_error_matrix_std_average.mean())\n",
    "print()\n",
    "\n",
    "# print(\"Distance-wise average: (closest)\")\n",
    "# plot_neighborhood_variability_matrix(distance_error_matrix_mean_closest)\n",
    "# print(\"Distance-wise average: (average)\")\n",
    "# plot_neighborhood_variability_matrix(distance_error_matrix_mean_average)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1e8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.dataset import LafasDataset, ThresholdSampler\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from src.utils.utils import get_patches, fix_prediction, fix_precition_average, compute_distance_error, compute_pointwise_error, finalize_landmark\n",
    "\n",
    "\n",
    "# ——— Usage ———\n",
    "dataset = LafasDataset(\n",
    "    root_dirs=[\"dataset\", \"validation_set\"],\n",
    "    # root_dirs=[\"sample\"],\n",
    "    cache_dir=\"cache(processed)_npz\"\n",
    ")\n",
    "\n",
    "sampler = ThresholdSampler(dataset, threshold=120.0, ref_idx=0)\n",
    "valid_indices = list(sampler)\n",
    "filtered_dataset = Subset(dataset, valid_indices)\n",
    "\n",
    "\n",
    "gt_landmarks = []\n",
    "for idx in filtered_dataset.indices:      # train_ds is a torch.utils.data.Subset\n",
    "    _, lm, _ = dataset[idx]       # dataset is your MeshLandmarkDataset\n",
    "    gt_landmarks.append(lm)\n",
    "\n",
    "gt_landmarks = np.array(gt_landmarks)  # Convert to numpy array\n",
    "gt_landmarks = finalize_landmark(gt_landmarks, True)\n",
    "gt_landmarks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c4371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each row\n",
    "def add_side_info_outside(text):\n",
    "    import re\n",
    "    # Extract sides and abbreviations\n",
    "    matches = re.findall(r'(Right|Left)?\\s*\\((.*?)\\)', text)\n",
    "    # Append \"R\" or \"L\" outside the parentheses based on the side\n",
    "    processed = [f'({abbr}){\"R\" if side == \"Right\" else \"L\" if side == \"Left\" else \"\"}' for side, abbr in matches]\n",
    "    return ' - '.join(processed)\n",
    "\n",
    "def get_named_pair_errors(landmark_pairs, landmark_names, distance_error_matrix):\n",
    "    results = []\n",
    "    results_dict = {}\n",
    "    for name1, name2 in landmark_pairs:\n",
    "        try:\n",
    "            idx1 = landmark_names.index(name1)\n",
    "            idx2 = landmark_names.index(name2)\n",
    "            error = distance_error_matrix[idx1, idx2]\n",
    "            results.append((name1, name2, error))\n",
    "            results_dict[(name1, name2)] = error\n",
    "        except ValueError as e:\n",
    "            print(f\"[ERROR] Landmark not found: {e}\")\n",
    "    \n",
    "    # # Optional: print summary\n",
    "    # print(f\"{'Landmark 1':<35} {'Landmark 2':<35} {'Error':.>10}\")\n",
    "    # print(\"-\" * 85)\n",
    "    # for name1, name2, err in results:\n",
    "    #     print(f\"{name1:<35} {name2:<35} {err:.4f}\")\n",
    "    \n",
    "    df = pd.DataFrame(results, columns=['Landmark 1', 'Landmark 2', 'Error'])\n",
    "    df['Landmark 1'] = df['Landmark 1'].apply(add_side_info_outside)\n",
    "    df['Landmark 2'] = df['Landmark 2'].apply(add_side_info_outside)\n",
    "    df['Landmark Pair'] = df['Landmark 1'] + ' — ' + df['Landmark 2']\n",
    "    df = df[['Landmark Pair', 'Error']]\n",
    "    return results, results_dict, df\n",
    "\n",
    "def compute_average_pairwise_distances(gt_landmarks, landmark_names, landmark_pairs):\n",
    "    distances = []\n",
    "    pair_labels = []\n",
    "\n",
    "    for name1, name2 in landmark_pairs:\n",
    "        try:\n",
    "            idx1 = landmark_names.index(name1)\n",
    "            idx2 = landmark_names.index(name2)\n",
    "\n",
    "            # Euclidean distance over all samples\n",
    "            dist = np.linalg.norm(gt_landmarks[:, idx1] - gt_landmarks[:, idx2], axis=1)\n",
    "            mean_dist = dist.mean()\n",
    "\n",
    "            label = f\"{name1} — {name2}\"\n",
    "            distances.append(mean_dist)\n",
    "            pair_labels.append(label)\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"[ERROR] Landmark not found: {e}\")\n",
    "            continue\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Landmark Pair': pair_labels,\n",
    "        'Mean Distance': distances\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "landmark_pairs = [\n",
    "# Frontal Distances\n",
    "['Trichion (Tr)' , 'Nasion (N)'],\n",
    "['Nasion (N)' , 'Pogonion (Pg)'],\n",
    "['Nasion (N)' , 'Subnasale (Sn)'],\n",
    "['Subnasale (Sn)' , 'Pogonion (Pg)'],\n",
    "\n",
    "#Horizontal plane\n",
    "['Exocanthion  Right (Ex)' , 'Exocanthion  Left (Ex)'],\n",
    "['Zygion  Right (Zy)', 'Zygion  Left (Zy)'],\n",
    "['Tragion  Right (T)', 'Tragion  Left (T)'],\n",
    "['Cheilion  Right (Ch)', 'Cheilion  Left (Ch)'],\n",
    "['Crista Philtri  Right (Cph)', 'Crista Philtri  Left (Cph)'],\n",
    "['Gonion  Right (Go)', 'Gonion  Left (Go)'],\n",
    "\n",
    "#Sagittal plane (Right)\n",
    "['Tragion  Right (T)', 'Nasion (N)'],\n",
    "['Tragion  Right (T)', 'Subnasale (Sn)'],\n",
    "['Tragion  Right (T)', 'Pogonion (Pg)'],\n",
    "['Pogonion (Pg)', 'Gonion  Right (Go)'],\n",
    "['Tragion  Right (T)', 'Gonion  Right (Go)'],\n",
    "\n",
    "#Sagittal plane (Left)\n",
    "['Tragion  Left (T)', 'Nasion (N)'],\n",
    "['Tragion  Left (T)', 'Subnasale (Sn)'],\n",
    "['Tragion  Left (T)', 'Pogonion (Pg)'],\n",
    "['Pogonion (Pg)', 'Gonion  Left (Go)'],\n",
    "['Tragion  Left (T)', 'Gonion  Left (Go)'],\n",
    "]\n",
    "\n",
    "Distance_variablity = [0.5884, 0.8903, 0.4449, 0.8405, 0.9393,\n",
    "       0.0816, 0.1922, 0.6520, 0.7380, 0.3285,\n",
    "       0.4593, 0.4159, 0.5443, 0.4873, 0.5425,\n",
    "       0.4281, 0.5106, 0.7003, 0.3846, 0.5038]\n",
    "\n",
    "# Run on your matrix (e.g., mean or std of average or closest)\n",
    "results, results_dict, df_error = get_named_pair_errors(landmark_pairs, landmark_names, distance_error_matrix_mean_average)\n",
    "\n",
    "\n",
    "df_distances_gt = compute_average_pairwise_distances(gt_landmarks, landmark_names, landmark_pairs)\n",
    "\n",
    "df_error['Mean Intra_variability'] = Distance_variablity\n",
    "\n",
    "\n",
    "df_error['avg_gt_distance'] = df_distances_gt['Mean Distance']\n",
    "df_error['Error_ratio'] = ((df_error['Error'] / df_error['avg_gt_distance']) * 100).apply(lambda x: f\"{x:.2f}%\")\n",
    "\n",
    "\n",
    "df_error = df_error[['Landmark Pair', 'Mean Intra_variability', 'Error', 'Error_ratio']]\n",
    "# df_error['Error'] = df_error['Error'].apply(lambda x: f\"{x:.2f} mm\")\n",
    "# df_error['Mean Intra_variability'] = df_error['Mean Intra_variability'].astype(str) + ' mm'\n",
    "\n",
    "# print(df_error.to_latex(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "\n",
    "# print(df_error['Error'].mean())\n",
    "# df_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfb0e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get Angles\n",
    "\n",
    "def compute_angle_between_landmarks(landmark_A, landmark_B, landmark_C):\n",
    "    \"\"\"\n",
    "    Computes the angle between three landmarks (A, B, C) using vector algebra.\n",
    "\n",
    "    Parameters:\n",
    "        landmark_A (numpy array): The coordinates of landmark A.\n",
    "        landmark_B (numpy array): The coordinates of landmark B.\n",
    "        landmark_C (numpy array): The coordinates of landmark C.\n",
    "\n",
    "    Returns:\n",
    "        float: The angle between landmarks A, B, and C in degrees.\n",
    "    \"\"\"\n",
    "    # Compute the vectors AB and BC\n",
    "    vector_AB = landmark_A - landmark_B\n",
    "    vector_BC = landmark_C - landmark_B\n",
    "\n",
    "    # Calculate the dot product and the norms of the vectors\n",
    "    dot_product = np.dot(vector_AB, vector_BC)\n",
    "    norm_AB = np.linalg.norm(vector_AB)\n",
    "    norm_BC = np.linalg.norm(vector_BC)\n",
    "\n",
    "    # Compute the cosine of the angle using the dot product formula\n",
    "    cos_angle = dot_product / (norm_AB * norm_BC)\n",
    "\n",
    "    # Ensure the cosine value is within the valid range of [-1, 1] to avoid numerical errors\n",
    "    cos_angle = np.clip(cos_angle, -1.0, 1.0)\n",
    "\n",
    "    # Compute the angle in radians and convert it to degrees\n",
    "    angle_radians = np.arccos(cos_angle)\n",
    "    angle_degrees = np.degrees(angle_radians)\n",
    "\n",
    "    return angle_degrees\n",
    "\n",
    "def compute_angle_error(gt_landmarks, predicted_landmarks, landmark_names, names_dict):\n",
    "    \"\"\"\n",
    "    Computes the angle error between three landmarks in ground truth and predicted coordinates.\n",
    "\n",
    "    Parameters:\n",
    "        gt_landmarks (numpy array): Ground truth landmarks of shape (40, 50, 3)\n",
    "        predicted_landmarks (numpy array): Predicted landmarks of shape (40, 50, 3)\n",
    "        landmark_names (tuple): A tuple of three names (A, B, C) for landmarks.\n",
    "        names_dict (dict): A dictionary mapping landmark names to indices.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            float: Angle error in degrees between the three landmarks.\n",
    "            float: Angle in degrees from the ground truth.\n",
    "    \"\"\"\n",
    "    # Extract the indices from the names\n",
    "    idx_A = names_dict[landmark_names[0]]\n",
    "    idx_B = names_dict[landmark_names[1]]\n",
    "    idx_C = names_dict[landmark_names[2]]\n",
    "\n",
    "    # Get the ground truth and predicted coordinates for the landmarks\n",
    "    gt_A = gt_landmarks[0, idx_A, :]\n",
    "    gt_B = gt_landmarks[0, idx_B, :]\n",
    "    gt_C = gt_landmarks[0, idx_C, :]\n",
    "\n",
    "    pred_A = predicted_landmarks[0, idx_A, :]\n",
    "    pred_B = predicted_landmarks[0, idx_B, :]\n",
    "    pred_C = predicted_landmarks[0, idx_C, :]\n",
    "\n",
    "    # Compute the angles for the ground truth and predictions\n",
    "    gt_angle = compute_angle_between_landmarks(gt_A, gt_B, gt_C)\n",
    "    pred_angle = compute_angle_between_landmarks(pred_A, pred_B, pred_C)\n",
    "\n",
    "    # Compute the absolute error between the angles\n",
    "    angle_error = np.abs(gt_angle - pred_angle)\n",
    "\n",
    "    return angle_error, gt_angle\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "def get_angle_df(y_test_par, preds_np_average_par):\n",
    "        \n",
    "    # Define mapping of landmark names to indices\n",
    "    names_dict = {name: idx for idx, name in enumerate(landmark_names)}\n",
    "\n",
    "    # Assume gt_landmarks and predicted_landmarks are your numpy arrays for ground truth and predicted landmarks\n",
    "    # gt_landmarks = finalize_landmark(y_test_par, True)\n",
    "    # predicted_landmarks = finalize_landmark(preds_np_average_par, True)\n",
    "\n",
    "    gt_landmarks = y_test_par\n",
    "    predicted_landmarks = preds_np_average_par\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    intra_var_angle = [0.095, 0.032, 0.345, 0.176, 1.435, 0.682, 0.228, 1.618, 0.676]\n",
    "\n",
    "    angles_to_calculate = [\n",
    "\n",
    "    ## Horizontal plane\n",
    "    ['Tragion  Right (T)', 'Nasion (N)', 'Tragion  Left (T)'],\n",
    "    ['Tragion  Right (T)', 'Pronasale (Prn)', 'Tragion  Left (T)'],\n",
    "    ['Tragion  Right (T)', 'Pogonion (Pg)', 'Tragion  Left (T)'],\n",
    "    ['Gonion  Right (Go)', 'Pogonion (Pg)', 'Gonion  Left (Go)'],\n",
    "\n",
    "    ## Sagittal plane (Right)\n",
    "    ['Nasion (N)', 'Subnasale (Sn)','Pogonion (Pg)'],\n",
    "    ['Nasion (N)', 'Pronasale (Prn)','Pogonion (Pg)'],\n",
    "    ['Subnasale (Sn)', 'Nasion (N)', 'Pronasale (Prn)'],\n",
    "    ['Tragion  Right (T)', 'Gonion  Right (Go)', 'Pogonion (Pg)'],\n",
    "    ['Tragion  Left (T)', 'Gonion  Left (Go)', 'Pogonion (Pg)'],\n",
    "\n",
    "    ]\n",
    "\n",
    "    mean_absolute_error = []\n",
    "    mean_absolute_error_ratio = []\n",
    "\n",
    "    for ang_landmarks in angles_to_calculate:\n",
    "        # print(ang_landmarks)\n",
    "        angle_error, gt_angle = compute_angle_error( gt_landmarks, predicted_landmarks, landmark_names=(ang_landmarks[0], ang_landmarks[1], ang_landmarks[2]), names_dict=names_dict)\n",
    "        mean_absolute_error.append(round(angle_error,2))\n",
    "        mean_absolute_error_ratio.append(round((angle_error/gt_angle),2))\n",
    "\n",
    "\n",
    "    df_angle = pd.DataFrame({\n",
    "        \"Angles\": [f\"{pair[0]} - {pair[1]} - {pair[2]}\" for pair in angles_to_calculate],\n",
    "        \"Mean Intra_variability\": intra_var_angle,\n",
    "        \"Mean Error\": mean_absolute_error,\n",
    "        \"Ratio\": mean_absolute_error_ratio\n",
    "    })\n",
    "\n",
    "    df_angle['Angles'] = df_angle['Angles'].apply(add_side_info_outside)\n",
    "\n",
    "    df_angle['Ratio'].mean()\n",
    "    # df_angle['Mean Error'].mean()\n",
    "    # df_angle\n",
    "    # print(df_angle.to_latex(index=False, float_format=\"%.2f\"))\n",
    "    return df_angle\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d346a6c",
   "metadata": {},
   "source": [
    "### to test per fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory of the current working directory to sys.path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from importlib import reload\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "\n",
    "# Set all seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)  # Sets seed for PyTorch CPU & CUDA operations\n",
    "    torch.cuda.manual_seed_all(seed)  # Sets seed for all CUDA devices (if used)\n",
    "    np.random.seed(seed)  # Sets seed for NumPy\n",
    "    random.seed(seed)  # Sets seed for Python's built-in random module\n",
    "    torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior\n",
    "    torch.backends.cudnn.benchmark = False  # Disables optimization that may introduce randomness\n",
    "set_seed(12345)  # Use the same seed as in the model\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, random_split, DataLoader\n",
    "\n",
    "from src.datasets.dataset import LafasDataset, mesh_collate_fn, ThresholdSampler\n",
    "from src.datasets.patch_dataset import PatchDataset, PatchDataset_sampled, PatchDataset_distance_based\n",
    "from src.utils.utils import get_patches, fix_prediction, fix_precition_average, compute_distance_error, compute_pointwise_error, compute_pointwise_error_std, compute_distance_error_std, finalize_landmark\n",
    "\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame()\n",
    "point_wise_closes = []\n",
    "point_wise_average = []\n",
    "distance_wise_closes = []\n",
    "distance_wise_average = []\n",
    "angle_dfs= []\n",
    "std_closes = []\n",
    "std_average = []\n",
    "\n",
    "std_closes_dist = []\n",
    "std_average_dist = []\n",
    "\n",
    "\n",
    "# ——— Usage ———\n",
    "dataset = LafasDataset(\n",
    "    root_dirs=[\"dataset\", \"validation_set\"],\n",
    "    # root_dirs=[\"sample\"],\n",
    "    cache_dir=\"cache(processed)_npz\"\n",
    ")\n",
    "\n",
    "\n",
    "# # 2.1) Filtering dataset (outliers of the registration process)\n",
    "sampler = ThresholdSampler(dataset, threshold=120.0, ref_idx=0)\n",
    "valid_indices = list(sampler)\n",
    "filtered_dataset = Subset(dataset, valid_indices)\n",
    "\n",
    "\n",
    "# # 2.2) Instead of filtering by ThresholdSampler, just use the full dataset:\n",
    "# filtered_dataset = dataset\n",
    "# valid_indices     = list(range(len(filtered_dataset)))   \n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# ——— Set up K-Fold ———\n",
    "\n",
    "K = 5\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "# storage for fold results\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(valid_indices), start=1):\n",
    "    print(f\"\\n==== Fold {fold}/{K} ====\")\n",
    "\n",
    "    # if fold != 5:\n",
    "    #     continue\n",
    "    \n",
    "    # create subsets for this fold\n",
    "    train_ds = Subset(filtered_dataset, train_idx)\n",
    "    val_ds   = Subset(filtered_dataset, val_idx)\n",
    "\n",
    "\n",
    "    ### average over the test set \n",
    "    sum_lm = torch.zeros(50, 3)\n",
    "    for idx in train_ds.indices:      # train_ds is a torch.utils.data.Subset\n",
    "        _, lm, _ = dataset[idx]       # dataset is your MeshLandmarkDataset\n",
    "        sum_lm += lm                  # lm is a (50,3) tensor\n",
    "    mean_lm = sum_lm / len(train_ds)\n",
    "    # print(mean_lm.shape)\n",
    "\n",
    "\n",
    "    train_ds_patches = PatchDataset(\n",
    "    # train_ds_patches = PatchDataset_distance_based(\n",
    "        base_ds    = train_ds,\n",
    "        mean_lm    = mean_lm,\n",
    "        patch_size = 1000,\n",
    "        # patch_size = 1500,\n",
    "        cache_dir  = \"patch_cache_train\"\n",
    "    )\n",
    "\n",
    "    val_ds_patches = PatchDataset(\n",
    "    # val_ds_patches = PatchDataset_distance_based(\n",
    "        base_ds    = val_ds,\n",
    "        mean_lm    = mean_lm,\n",
    "        patch_size = 1000,\n",
    "        # patch_size = 1500,\n",
    "        cache_dir  = \"patch_cache_test\"\n",
    "    )\n",
    "\n",
    "\n",
    "    train_loader_patches = DataLoader(train_ds_patches,\n",
    "                            batch_size=16,\n",
    "                            shuffle=False,      # shuffling here is OK\n",
    "                            num_workers=4,\n",
    "                            pin_memory=True,\n",
    "                            )\n",
    "\n",
    "\n",
    "    val_loader_patches = DataLoader(val_ds_patches,\n",
    "                            batch_size=16,\n",
    "                            shuffle=False,      # shuffling here is OK\n",
    "                            num_workers=4,\n",
    "                            pin_memory=True,\n",
    "                            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Pre‐caching patches (Train_set)…\")\n",
    "    for i in tqdm(range(len(train_ds_patches)), desc=\"Caching patches\"):\n",
    "        _ = train_ds_patches[i]\n",
    "\n",
    "    print(\"Pre‐caching patches (Validation Set)…\")\n",
    "    for i in tqdm(range(len(val_ds_patches)), desc=\"Caching patches\"):\n",
    "        _ = val_ds_patches[i]\n",
    "\n",
    "    for data in train_loader_patches:\n",
    "        X_patch_sample, y_landmark, X_sampled_sample = data\n",
    "        break\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------\n",
    "\n",
    "    # ——— Set up Model ———\n",
    "    from src.models.model import PALNET, PLNET_noatt, PALNET_topk, PALNET_2blk\n",
    "    from src.models.loss import CombinedLoss, localizationLoss \n",
    "    import torch.optim as optim\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    input_shape = X_patch_sample[0].shape\n",
    "    output_shape = y_landmark[0].shape\n",
    "\n",
    "    # Define model\n",
    "    model = PALNET(input_shape, output_shape, seed=42).to(device, non_blocking=True)\n",
    "\n",
    "    #### for ab study\n",
    "    # model = PLNET_noatt(input_shape, output_shape, seed=42).to(device, non_blocking=True)\n",
    "    # model = PALNET_topk(input_shape, output_shape, seed=42).to(device, non_blocking=True)\n",
    "    # model = PALNET_2blk(input_shape, output_shape, seed=42).to(device, non_blocking=True)\n",
    "\n",
    "    # Define loss function & optimizer\n",
    "    criterion = CombinedLoss(alpha=0.6, beta=0.4)  \n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Training Loop\n",
    "    num_epochs = 5000  # Adjust as needed\n",
    "    model.to(device, non_blocking=True)\n",
    "\n",
    "    # Early Stopping Parameters\n",
    "    patience = 30 \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ——— Training Loop ———\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=8, verbose=True)\n",
    "\n",
    "    # for epoch in range(num_epochs):\n",
    "    #     model.train()  # Set model to training mode\n",
    "    #     running_loss = 0.0\n",
    "\n",
    "    #     ## training loop\n",
    "    #     for X_patches, y_landmarks, _ in train_loader_patches:\n",
    "\n",
    "    #         X_patches = X_patches.to(device, non_blocking=True)\n",
    "    #         y_landmarks = y_landmarks.to(device, non_blocking=True)\n",
    "\n",
    "    #         optimizer.zero_grad()  # Zero gradients\n",
    "    #         outputs = model(X_patches)  # Forward pass\n",
    "    #         loss = criterion(outputs, y_landmarks)  # Compute loss\n",
    "    #         loss.backward()  # Backpropagation\n",
    "    #         optimizer.step()  # Update weights\n",
    "\n",
    "    #         running_loss += loss.item() * X_patches.size(0)\n",
    "    #     train_loss = running_loss / len(train_ds)\n",
    "\n",
    "    #     # Validation Loop\n",
    "    #     model.eval()  # Set model to evaluation mode\n",
    "    #     val_loss = 0.0\n",
    "    #     with torch.no_grad():  # No gradients needed for validation\n",
    "    #         for X_val, y_val, _ in val_loader_patches:\n",
    "    #             X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "    #             val_outputs = model(X_val)\n",
    "    #             val_loss += criterion(val_outputs, y_val).item() * X_val.size(0)\n",
    "\n",
    "    #     val_loss /= len(val_ds)\n",
    "    #     scheduler.step(val_loss)\n",
    "        \n",
    "    #     # Print Epoch Summary\n",
    "    #     print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    #     # Early Stopping\n",
    "    #     if val_loss < best_val_loss:\n",
    "    #         best_val_loss = val_loss\n",
    "    #         epochs_no_improve = 0\n",
    "    #         bestmodel = model.state_dict()\n",
    "    #         if epoch > 50:\n",
    "    #             torch.save(model.state_dict(), \"best_model_ref\"+str(fold) +\".pth\")  # Save best model\n",
    "    #     else:\n",
    "    #         epochs_no_improve += 1\n",
    "    #         if epochs_no_improve >= patience:\n",
    "    #             print(f\"Early stopping at epoch {epoch+1}\")\n",
    "    #             break  # Stop training if no improvement for `patience` epochs\n",
    "        \n",
    "    #     print(\"best_val_loss: \", best_val_loss)\n",
    "\n",
    "    # Load Best Model\n",
    "    # model.load_state_dict(torch.load(\"best_model_ref\"+str(fold) +\".pth\"))\n",
    "    model.load_state_dict(torch.load(\"5-fold-cv/best_model_ref\"+str(fold) +\".pth\"))\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    print(\"Best model loaded!\")\n",
    "    # Save the model for this fold\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------\n",
    "    # ——— Evaluation Loop ———\n",
    "    pred = []\n",
    "    y_test = []\n",
    "    X_test = []\n",
    "    X_test_raw = []\n",
    "\n",
    "    for X_patches, y_landmarks, _ in val_loader_patches:\n",
    "        X_patches = X_patches.to(device, non_blocking=True)\n",
    "        y_landmarks = y_landmarks.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_patches)\n",
    "            pred.append(outputs.cpu().numpy())\n",
    "            y_test.append(y_landmarks.cpu().numpy())\n",
    "            X_test.append(_.cpu().numpy())\n",
    "            X_test_raw.append(X_patches.cpu().numpy())\n",
    "\n",
    "    pred = np.concatenate(pred, axis=0)\n",
    "    y_test = np.concatenate(y_test, axis=0)\n",
    "    X_test = np.concatenate(X_test, axis=0)\n",
    "    X_test_raw = np.concatenate(X_test_raw, axis=0)\n",
    "    X_test_raw = X_test_raw.reshape(X_test_raw.shape[0], -1, 3)\n",
    "\n",
    "\n",
    "\n",
    "    preds_np_closes = fix_prediction(X_test, pred.copy())\n",
    "    preds_np_average = fix_precition_average(X_test, pred.copy(), k =12)\n",
    "\n",
    "\n",
    "    y_test = finalize_landmark(y_test.copy(), True)\n",
    "    preds_np_closes = finalize_landmark(preds_np_closes.copy(), True)\n",
    "    preds_np_average = finalize_landmark(preds_np_average.copy(), True)\n",
    "\n",
    "\n",
    "    # Compute distance error\n",
    "    distance_error_closes  = compute_distance_error(y_test, preds_np_closes)\n",
    "    distance_error_average = compute_distance_error(y_test, preds_np_average)\n",
    "\n",
    "    err_closes_std  = compute_distance_error_std(y_test, preds_np_closes)\n",
    "    err_average_std = compute_distance_error_std(y_test, preds_np_average)\n",
    "\n",
    "\n",
    "\n",
    "    err_closes  = compute_pointwise_error(preds_np_closes, y_test)\n",
    "    err_average = compute_pointwise_error(preds_np_average, y_test)\n",
    "\n",
    "    std_closes.append(compute_pointwise_error_std(preds_np_closes, y_test))\n",
    "    std_average.append(compute_pointwise_error_std(preds_np_average, y_test))\n",
    "    \n",
    "\n",
    "\n",
    "    print(\"___________ Evaluation \"+str(fold)+\"___________\")\n",
    "    print(\"Pair-wise_closes:\" , distance_error_closes.mean())\n",
    "    print(\"Pair-wise_average:\" , distance_error_average.mean())\n",
    "    print()\n",
    "    print(\"Point-wise_closes:\" ,err_closes.mean())\n",
    "    print(\"Point-wise_average:\" ,err_average.mean())\n",
    "\n",
    "    \n",
    "\n",
    "    df_results[f\"Fold_{fold}_Point-wise_closes\"] = err_closes\n",
    "    df_results[f\"Fold_{fold}_Point-wise_average\"] = err_average\n",
    "    \n",
    "\n",
    "\n",
    "    point_wise_closes.append(err_closes)\n",
    "    point_wise_average.append(err_average)\n",
    "    distance_wise_closes.append(distance_error_closes)\n",
    "    distance_wise_average.append(distance_error_average)\n",
    "\n",
    "    # Save results to CSV\n",
    "    path_to_save = \"results_\"+str(fold) +\".csv\"\n",
    "    # df_results.to_csv(path_to_save, mode='a', header=False, index=False)\n",
    "\n",
    "\n",
    "    angle_dfs.append(get_angle_df(y_test, preds_np_average))\n",
    "\n",
    "\n",
    "    import shutil\n",
    "    shutil.rmtree(\"patch_cache_train\")\n",
    "    shutil.rmtree(\"patch_cache_test\")\n",
    "    # break\n",
    "\n",
    "\n",
    "\n",
    "point_wise_closes = np.array(point_wise_closes)\n",
    "point_wise_average = np.array(point_wise_average)\n",
    "distance_wise_closes = np.array(distance_wise_closes)\n",
    "distance_wise_average = np.array(distance_wise_average)\n",
    "\n",
    "\n",
    "# np.save(\"point_wise_closes.npy\", point_wise_closes)\n",
    "# np.save(\"point_wise_average.npy\", point_wise_average)\n",
    "# np.save(\"distance_wise_closes.npy\", distance_wise_closes)\n",
    "# np.save(\"distance_wise_average.npy\", distance_wise_average)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156730e5",
   "metadata": {},
   "outputs": [],
   "source": [
    " y_test = finalize_landmark(y_test.copy(), True)\n",
    "    preds_np_closes = finalize_landmark(preds_np_closes.copy(), True)\n",
    "    preds_np_average = finalize_landmark(preds_np_average.copy(), True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0064a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a parallel list of 'midline'/'right'/'left'\n",
    "group_per_lm = []\n",
    "for nm in landmark_names:\n",
    "    if 'Right' in nm:\n",
    "        group_per_lm.append('right')\n",
    "    elif 'Left' in nm:\n",
    "        group_per_lm.append('left')\n",
    "    else:\n",
    "        group_per_lm.append('midline')\n",
    "\n",
    "# colour map\n",
    "colors = {'midline':'blue', 'right':'red', 'left':'green'}\n",
    "\n",
    "def bland_altman_by_group(gt, pr, lm_groups, colors):\n",
    "    \"\"\"\n",
    "    gt, pr: (N_samples, N_landmarks=50, 3)\n",
    "    lm_groups: list of length 50 with 'midline'/'right'/'left'\n",
    "    colors: dict mapping group → colour\n",
    "    \"\"\"\n",
    "    # sanity\n",
    "    N, L, D = gt.shape\n",
    "    assert D == 3\n",
    "    assert len(lm_groups) == L\n",
    "    assert pr.shape == gt.shape\n",
    "\n",
    "    # compute mean & diff arrays (shape (N,L,3))\n",
    "    mean_arr = (gt + pr) / 2\n",
    "    diff_arr = pr - gt\n",
    "\n",
    "    # flatten to 1D arrays of length N*L*3\n",
    "    mean_flat = mean_arr.flatten()\n",
    "    diff_flat = diff_arr.flatten()\n",
    "\n",
    "    # build a flat group‐label array of same length\n",
    "    # 1) make shape (1,L,1) from lm_groups, then tile to (N,L,3)\n",
    "    grp = np.array(lm_groups)[None,:,None]\n",
    "    grp = np.tile(grp, (N, 1, 3))\n",
    "    grp_flat = grp.flatten()\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for g in ['midline','right','left']:\n",
    "        m = (grp_flat == g)\n",
    "        plt.scatter(mean_flat[m], diff_flat[m],\n",
    "                    alpha=0.5, color=colors[g], label=g.capitalize())\n",
    "\n",
    "    # bias lines\n",
    "    m0 = diff_flat.mean()\n",
    "    sd = diff_flat.std()\n",
    "    for val, lab in [(m0,       f'Mean diff: {m0:.2f}'),\n",
    "                     (m0+1.96*sd, f'+1.96 SD: {m0+1.96*sd:.2f}'),\n",
    "                     (m0-1.96*sd, f'-1.96 SD: {m0-1.96*sd:.2f}')]:\n",
    "        plt.axhline(val, color='gray', linestyle='--', label=lab)\n",
    "\n",
    "    plt.xlabel('Mean coordinate (X/Y/Z)')\n",
    "    plt.ylabel('Difference (Pred – GT)')\n",
    "    plt.title('Bland–Altman by Landmark Group')\n",
    "    plt.legend(bbox_to_anchor=(1.02,1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# finally, call it with your arrays:\n",
    "# ground_truth and predicted must be shape (n_samples, 50, 3)\n",
    "bland_altman_by_group(ground_truth, predicted, group_per_lm, colors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torcheroids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
